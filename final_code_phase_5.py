# -*- coding: utf-8 -*-
"""Final_code_Phase_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12J7x-wQFkGFasyS4yWJ3NJiCUiG-ed0T
"""

# Dataset 1: Heart Failure Prediction Dataset

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# This is the basic Structure of the dataset
dataset1 = pd.read_csv('/content/heart_failure_prediction_dataset.csv')
print(dataset1.info())

# This is the summary statistics
print(dataset1.describe())

# Here are the frequency of categorical data for the below attributes
for column in ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']:
    print(f"\nFrequency of categories in {column}:")
    print(dataset1[column].value_counts())

sns.set_style("whitegrid")

# Here are the histograms for the numerical data
dataset1.hist(bins=15, figsize=(15, 10), layout=(4, 2))
plt.show()

# Here are the boxplots for each numerical feature to check for outliers
num_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']
for column in num_columns:
    plt.figure(figsize=(3, 3))
    sns.boxplot(x=dataset1[column])
    plt.show()

# This is the count plots for the categorical features
cat_columns = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']
for column in cat_columns:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=column, data=dataset1)
    plt.show()

# Here it is checking the duplicates
print(f"Number of duplicate entries: {dataset1.duplicated().sum()}")

# Here it is checking the negative values
print(f"Negative values in Age: {dataset1[dataset1['Age'] < 0].shape[0]}")
print(f"Negative values in RestingBP: {dataset1[dataset1['RestingBP'] < 0].shape[0]}")
print(f"Negative values in Cholesterol: {dataset1[dataset1['Cholesterol'] < 0].shape[0]}")

grouped = dataset1.groupby('HeartDisease')
print(grouped[num_columns].mean())

# Here are the visual comparisons
for column in num_columns:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x='HeartDisease', y=column, data=dataset1)
    plt.show()

# Dataset 2: Heart Disease Dataset

dataset2 = pd.read_csv('heart_disease_dataset.csv')
dataset2_cleaned = dataset2.drop_duplicates()
# This is the basic Structure of the dataset
print(dataset2.info())

# This is the summary statistics
print(dataset2.describe())

# Here are the frequency of categorical data for the below attributes
for column in ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']:
    print(f"\nFrequency of categories in {column}:")
    print(dataset2[column].value_counts())

# Here are the histograms for the numerical data
dataset2.hist(bins=15, figsize=(15, 10), layout=(5, 3))
plt.show()

# Here are the boxplots for each numerical feature to check for outliers
num_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
for column in num_columns:
    plt.figure(figsize=(5, 5))
    sns.boxplot(x=dataset2[column])
    plt.show()

# This is the count plots for the categorical features
cat_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
for column in cat_columns:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=column, data=dataset2)
    plt.show()

# Here it is checking the duplicates
print(f"Number of duplicate entries: {dataset2_cleaned.duplicated().sum()}")

# Here it is checking the negative values
print(f"Negative or zero values in 'trestbps': {dataset2[dataset2['trestbps'] <= 0].shape[0]}")
print(f"Negative or zero values in 'chol': {dataset2[dataset2['chol'] <= 0].shape[0]}")

# Here it is Comparing the numerical features by target (heart disease presence)
grouped_target = dataset2.groupby('target')
print(grouped_target[num_columns].mean())

# Here are the visual comparisons
for column in num_columns:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x='target', y=column, data=dataset2)
    plt.show()

# Dataset 3: Cleveland.data

# Here it is defined the column names based on the dataset documentation
columns = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
]

# This is the basic Structure of the dataset
cleveland_path = 'processed.cleveland.data'
dataset3 = pd.read_csv(cleveland_path, names=columns)

print(dataset3.info())

# This is the summary statistics
print(dataset3.describe())

# Here are the frequency of categorical data for the below attributes
for column in ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']:
    print(f"\nFrequency of categories in {column}:")
    print(dataset3[column].value_counts())

# Here are the histograms for the numerical data
dataset3.hist(bins=15, figsize=(15, 10), layout=(5, 3))
plt.show()

# Here are the boxplots for each numerical feature to check for outliers
num_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
for column in num_columns:
    plt.figure(figsize=(5, 5))
    sns.boxplot(x=dataset3[column])
    plt.show()

# This is the count plots for the categorical features
cat_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
for column in cat_columns:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=column, data=dataset3)
    plt.show()

# Here it is checking the duplicates
print(f"Number of duplicate entries: {dataset3.duplicated().sum()}")

dataset3['ca'] = dataset3['ca'].replace('?', '0')
dataset3['thal'] = dataset3['thal'].replace('?', '0')

dataset3['ca'] = pd.to_numeric(dataset3['ca'])
dataset3['thal'] = pd.to_numeric(dataset3['thal'])

# Here it is checking for the missing values
print(f"Missing values in 'ca': {dataset3[dataset3['ca'].isnull()].shape[0]}")
print(f"Missing values in 'thal': {dataset3[dataset3['thal'].isnull()].shape[0]}")

grouped_target = dataset3.groupby('target')
print(grouped_target[num_columns].mean())

# Here are the visual comparisons
for column in num_columns:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x='target', y=column, data=dataset3)
    plt.show()

# After data visualization we are combining the cleveland.data and heart_disease_dataset. With that combined dataset we showed results using this:
# data visualization
# data Preprocessing
# data splitting
# data normalization
# using the base classifiers models
# printing ROC curves for all models
# printing confusion matrix for all models
# Hyperparameter Tuning with GridSearchCV for Various Classifiers
# Including the Stacking Classifier and ROC Curves
# Evaluating all models with varying parameter sets
# K-Fold Cross-Validation for Multiple Classifiers and plot

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import roc_curve, auc
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import warnings
warnings.filterwarnings('ignore')

column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang',
    'oldpeak', 'slope', 'ca', 'thal', 'target'
]

# Loading the datasets
cleveland_data = pd.read_csv('/content/processed.cleveland.data', names=column_names, header=None)
heart_disease_data = pd.read_csv('/content/heart_disease_dataset.csv', header=0)

# Combining the datasets
combined_data = pd.concat([cleveland_data, heart_disease_data], ignore_index=True)
combined_data = combined_data.dropna()
print(combined_data.head())

combined_data.to_csv('combined_dataset.csv', index=False)

print(combined_data.info())

print(combined_data.describe())

for column in ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']:
    print(f"\nFrequency of categories in {column}:")
    print(combined_data[column].value_counts())

combined_data.hist(bins=15, figsize=(15, 10), layout=(5, 3))
plt.show()

# Here are the boxplots for each numerical feature to check for outliers
num_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
for column in num_columns:
    plt.figure(figsize=(5, 5))
    sns.boxplot(x=combined_data[column])
    plt.show()

# This is the count plots for the categorical features
cat_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
for column in cat_columns:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=column, data=combined_data)
    plt.show()

grouped_target = combined_data.groupby('target')
print(grouped_target[num_columns].mean())

combined_data = combined_data.apply(pd.to_numeric, errors='coerce')
combined_data.dropna(inplace=True)
correlation_matrix = combined_data.corr()

# Printing the correlation matrix
print(correlation_matrix)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True, square=True)
plt.title('Correlation Matrix')
plt.show()

columns_of_interest = ['age', 'sex', 'cp', 'chol', 'target']
filtered_correlation_matrix = correlation_matrix.loc[columns_of_interest, columns_of_interest]

# Printing the filtered correlation matrix
print(filtered_correlation_matrix)

# here printing plot for the filtered correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(filtered_correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True, square=True)
plt.title('Filtered Correlation Matrix')
plt.show()

combined_data.replace('?', np.nan, inplace=True)

for column in combined_data.columns:
    combined_data[column] = pd.to_numeric(combined_data[column], errors='coerce')

combined_data.dropna(inplace=True)
X = combined_data.drop('target', axis=1)
y = combined_data['target']

# Spliting the data into training and temporary set
# Spliting the temporary set into validation and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)

# here using the Data Normalization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initializing the classifiers
logistic_model = LogisticRegression()
tree_model = DecisionTreeClassifier(random_state=42)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)
svm_model = SVC(kernel='linear')
neural_network_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)
knn_model = KNeighborsClassifier(n_neighbors=5)
nb_model = GaussianNB()
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

svm_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=5)
nn_scores = cross_val_score(neural_network_model, X_train_scaled, y_train, cv=5)

# Fiting the models
logistic_model.fit(X_train, y_train)
tree_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
adaboost_model.fit(X_train, y_train)
svm_model.fit(X_train_scaled, y_train)
neural_network_model.fit(X_train_scaled, y_train)
knn_model.fit(X_train, y_train)
nb_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# predicting for the each model
predictions_logistic = logistic_model.predict(X_test)
predictions_tree = tree_model.predict(X_test)
predictions_rf = rf_model.predict(X_test)
predictions_adaboost = adaboost_model.predict(X_test)
predictions_svm = svm_model.predict(X_test_scaled)
nn_model_predictions = neural_network_model.predict(X_test_scaled)
knn_predictions = knn_model.predict(X_test)
nb_predictions = nb_model.predict(X_test)
xgb_predictions = xgb_model.predict(X_test)

# Printing the evaluation metrics
def evaluate_model(y_true, predictions, model_name):
    print(f"Model: {model_name}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, predictions))
    print("Classification Report:")
    print(classification_report(y_true, predictions))
    print("Accuracy:", accuracy_score(y_true, predictions))
    print("Mean Squared Error (MSE):", mean_squared_error(y_true, predictions))
    print("R² Score:", r2_score(y_true, predictions))

evaluate_model(y_test, predictions_logistic, 'Logistic Regression')

evaluate_model(y_test, predictions_tree, 'Decision Tree Classifier')

evaluate_model(y_test, predictions_rf, 'Random Forest Classifier')

evaluate_model(y_test, predictions_adaboost, 'AdaBoost Classifier')

evaluate_model(y_test, predictions_svm, 'Support Vector Machines')

evaluate_model(y_test, nn_model_predictions, 'Neural Networks')

evaluate_model(y_test, knn_predictions, 'K-Nearest Neighbours')

evaluate_model(y_test, nb_predictions, 'Naive Bayes')

evaluate_model(y_test, xgb_predictions, 'Extreme Gradient Boosting')

def train_evaluate_model(model, X_train, y_train, X_test, y_test, model_name):

    start_train_time = time.time()
    model.fit(X_train, y_train)
    end_train_time = time.time()
    training_time = end_train_time - start_train_time

    start_test_time = time.time()
    predictions = model.predict(X_test)
    end_test_time = time.time()
    testing_time = end_test_time - start_test_time

    print(f"Model: {model_name}")
    print(f"Training Time: {training_time:.4f} seconds")
    print(f"Testing Time: {testing_time:.4f} seconds")

train_evaluate_model(logistic_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Logistic Regression')

train_evaluate_model(tree_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Decision Tree Classifier')

train_evaluate_model(rf_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Random Forest Classifier')

train_evaluate_model(adaboost_model, X_train_scaled, y_train, X_test_scaled, y_test, 'AdaBoost Classifier')

train_evaluate_model(svm_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Support Vector Machines')

train_evaluate_model(neural_network_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Neural Networks')

train_evaluate_model(knn_model, X_train_scaled, y_train, X_test_scaled, y_test, 'K-Nearest Neighbours')

train_evaluate_model(nb_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Naive Bayes')

train_evaluate_model(xgb_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Extreme Gradient Boosting')

models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'Neural Network', 'KNN', 'Naive Bayes', 'XGBoost']
accuracies = [0.67, 0.86, 0.86, 0.46, 0.67, 0.83, 0.53, 0.64, 0.84]

plt.figure(figsize=(10, 5))
sns.barplot(x=models, y=accuracies, palette='viridis')
plt.title('Comparison of Model Accuracies')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.show()

models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'Neural Network', 'KNN', 'Naive Bayes', 'XGBoost']
mse_scores = [0.5, 0.47, 0.30, 0.82, 0.48, 0.35, 0.86, 0.53, 0.33]
r2_scores = [0.03, 0.10, 0.42, -0.56, 0.08, 0.33, -0.64, -0.01, 0.36]

plt.figure(figsize=(10, 5))
plt.plot(models, mse_scores, marker='o', linestyle='-', color='b', label='MSE Score')
plt.title('MSE Scores by Model')
plt.xlabel('Model')
plt.ylabel('MSE Score')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(models, r2_scores, marker='o', linestyle='-', color='g', label='R² Score')
plt.title('R² Scores by Model')
plt.xlabel('Model')
plt.ylabel('R² Score')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'Neural Network', 'KNN', 'Naive Bayes', 'XGBoost']
training_times = [0.0382, 0.0071, 0.2152, 0.2377, 0.0863, 2.6600, 0.0040, 0.0035, 0.1658]
testing_times = [0.0004, 0.0005, 0.0100, 0.0181, 0.0086, 0.0008, 0.0210, 0.0022, 0.0041]

plt.figure(figsize=(12, 6))
plt.plot(models, training_times, marker='o', linestyle='-', color='black', label='Training Time')
plt.plot(models, testing_times, marker='s', linestyle='--', color='orange', label='Testing Time')
plt.title('Training and Testing Times for Various Models')
plt.xlabel('Model')
plt.ylabel('Time (seconds)')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.show()

def plot_multiclass_roc_curve(models, X_test, y_test):
    y_test_binarized = label_binarize(y_test, classes=[0,1])
    n_classes = y_test_binarized.shape[1]

    plt.figure(figsize=(10, 8))

    for model_name, model in models.items():
        if hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X_test)
        else:
            y_score = model.decision_function(X_test)

        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f'{model_name} (Class {i} AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Different Models')
    plt.legend(loc="lower right")
    plt.grid()
    plt.show()

models_dict = {
    'XGBoost': xgb_model,
    'Logistic Regression': logistic_model,
    'Decision Tree Classifier': tree_model,
    'Random Forest Classifier': rf_model,
    'K-Nearest Neighbours': knn_model,
    'Neural Networks': neural_network_model,
    'Support Vector Machines': svm_model,
    'AdaBoost Classifier': adaboost_model
}

plot_multiclass_roc_curve(models_dict, X_test_scaled, y_test)

# Function to plot only the confusion matrix
def plot_confusion_matrix_only(model, X_test, y_test, model_name='Model'):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)
    plt.title(f'Confusion Matrix: {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

classifiers = [
    (logistic_model, "Logistic Regression"),
    (tree_model, "Decision Tree"),
    (rf_model, "Random Forest"),
    (adaboost_model, "AdaBoost"),
    (svm_model, "Support Vector Machine"),
    (neural_network_model, "Neural Network"),
    (knn_model, "K-Nearest Neighbours"),
    (nb_model, "Naive Bayes"),
    (xgb_model, "XGBoost")
]

for model, name in classifiers:
    X_test_data = X_test_scaled if name in ['Support Vector Machine', 'Neural Network'] else X_test
    plot_confusion_matrix_only(model, X_test_data, y_test, model_name=name)

base_models = [
    ('logistic', LogisticRegression(max_iter=1000)),
    ('tree', DecisionTreeClassifier(random_state=42)),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('adaboost', AdaBoostClassifier(n_estimators=100, random_state=42)),
    ('svm', SVC(kernel='linear', probability=True)),
    ('neural_net', MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)),
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('nb', GaussianNB()),
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))
]

meta_model = LogisticRegression()
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)
stacking_model.fit(X_train_scaled, y_train)
stacking_predictions = stacking_model.predict(X_test_scaled)

evaluate_model(y_test, stacking_predictions, 'Stacking Classifier')

models_dict = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_scaled, y_train),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss').fit(X_train_scaled, y_train),
    'Stacking Classifier': stacking_model
}

y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_binarized.shape[1]

plt.figure(figsize=(10, 8))

for model_name, model in models_dict.items():
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test_scaled)
    else:
        y_score = model.decision_function(X_test_scaled)

    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (Class {i} AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Different Models')
plt.legend(loc="lower right")
plt.grid()
plt.show()

logistic_param_sets = [
    {'C': 1.0, 'solver': 'lbfgs'},
    {'C': 0.5, 'solver': 'liblinear'},
    {'C': 2.0, 'solver': 'saga'}
]

tree_param_sets = [
    {'max_depth': 10, 'min_samples_split': 2},
    {'max_depth': 20, 'min_samples_split': 4},
    {'max_depth': None, 'min_samples_split': 10}
]

rf_param_sets = [
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 20},
    {'n_estimators': 300, 'max_depth': None}
]

adaboost_param_sets = [
    {'n_estimators': 50, 'learning_rate': 1.0},
    {'n_estimators': 100, 'learning_rate': 0.5},
    {'n_estimators': 200, 'learning_rate': 0.1}
]

svm_param_sets = [
    {'kernel': 'linear', 'C': 1.0},
    {'kernel': 'rbf', 'C': 0.5, 'gamma': 'scale'},
    {'kernel': 'poly', 'C': 2.0, 'degree': 3}
]

nn_param_sets = [
    {'hidden_layer_sizes': (100,), 'max_iter': 200},
    {'hidden_layer_sizes': (50, 50), 'max_iter': 500},
    {'hidden_layer_sizes': (100, 50), 'max_iter': 1000}
]

knn_param_sets = [
    {'n_neighbors': 5},
    {'n_neighbors': 10},
    {'n_neighbors': 20}
]

nb_param_sets = [
    {'var_smoothing': 1e-9},
    {'var_smoothing': 1e-8},
    {'var_smoothing': 1e-7}
]

xgb_param_sets = [
    {'n_estimators': 100, 'max_depth': 3},
    {'n_estimators': 200, 'max_depth': 6},
    {'n_estimators': 300, 'max_depth': 10}
]

def evaluate_param_sets(model_class, param_sets, model_name, scaled=True):
    results = []
    for params in param_sets:
        model = model_class(**params)

        start_train_time = time.time()
        if scaled:
            model.fit(X_train_scaled, y_train)
            predictions = model.predict(X_test_scaled)
        else:
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)
        end_train_time = time.time()
        training_time = end_train_time - start_train_time

        start_test_time = time.time()
        end_test_time = time.time()
        testing_time = end_test_time - start_test_time

        accuracy = accuracy_score(y_test, predictions)
        mse = mean_squared_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)

        results.append({
            'Parameters': params,
            'Accuracy': accuracy,
            'MSE': mse,
            'R² Score': r2,
            'Training Time': training_time,
            'Testing Time': testing_time
        })

    results_df = pd.DataFrame(results)
    print(f"Results for {model_name}:")
    print(results_df)

    results_df.plot(x='Parameters', y=['Accuracy', 'MSE', 'R² Score'], kind='bar', figsize=(12, 6), title=f'{model_name} Metrics by Parameter Set')
    plt.xticks(rotation=0)
    plt.grid(axis='y')
    plt.show()

evaluate_param_sets(LogisticRegression, logistic_param_sets, 'Logistic Regression', scaled=True)

evaluate_param_sets(DecisionTreeClassifier, tree_param_sets, 'Decision Tree', scaled=False)

evaluate_param_sets(RandomForestClassifier, rf_param_sets, 'Random Forest', scaled=False)

evaluate_param_sets(AdaBoostClassifier, adaboost_param_sets, 'AdaBoost', scaled=False)

evaluate_param_sets(SVC, svm_param_sets, 'Support Vector Machine', scaled=True)

evaluate_param_sets(MLPClassifier, nn_param_sets, 'Neural Network', scaled=True)

evaluate_param_sets(KNeighborsClassifier, knn_param_sets, 'K-Nearest Neighbours', scaled=False)

evaluate_param_sets(GaussianNB, nb_param_sets, 'Naive Bayes', scaled=False)

evaluate_param_sets(XGBClassifier, xgb_param_sets, 'Extreme Gradient Boosting', scaled=True)

# Hyperparameter Tuning with GridSearchCV for Various Classifiers
# This code performs hyperparameter tuning for various classification models using grid search with cross-validation.
# For each model, it defines a parameter grid specifying different hyperparameter values to explore.
# It uses GridSearchCV to search through the parameter grid using 5-fold cross-validation and optimize for accuracy.
# After fitting the grid search object to the training data, it prints the best parameters found for each model and the corresponding best cross-validation score.

param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear']
}

grid_search_logistic = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_logistic, cv=5, scoring='accuracy')
grid_search_logistic.fit(X_train, y_train)

param_grid_tree = {
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'criterion': ['gini', 'entropy']
}

grid_search_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_tree, cv=5, scoring='accuracy')
grid_search_tree.fit(X_train, y_train)

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10]
}

grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train, y_train)

param_grid_adaboost = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}

grid_search_adaboost = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid_adaboost, cv=5, scoring='accuracy')
grid_search_adaboost.fit(X_train, y_train)

param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

grid_search_svm = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5, scoring='accuracy')
grid_search_svm.fit(X_train, y_train)

param_grid_mlp = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01]
}

grid_search_mlp = GridSearchCV(MLPClassifier(max_iter=1000), param_grid_mlp, cv=5, scoring='accuracy')
grid_search_mlp.fit(X_train, y_train)

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')
grid_search_knn.fit(X_train, y_train)

param_grid_nb = {
    'var_smoothing': np.logspace(-9, -3, 7)
}

grid_search_nb = GridSearchCV(GaussianNB(), param_grid_nb, cv=5, scoring='accuracy')
grid_search_nb.fit(X_train, y_train)

param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid_search_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), param_grid_xgb, cv=5, scoring='accuracy')
grid_search_xgb.fit(X_train, y_train)

print("Best Parameters (Logistic Regression):", grid_search_logistic.best_params_)
print("Best Score:", grid_search_logistic.best_score_)
print("Best Parameters (Decision Tree):", grid_search_tree.best_params_)
print("Best Score:", grid_search_tree.best_score_)
print("Best Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Score:", grid_search_rf.best_score_)
print("Best Parameters (AdaBoost):", grid_search_adaboost.best_params_)
print("Best Score:", grid_search_adaboost.best_score_)
print("Best Parameters (SVM):", grid_search_svm.best_params_)
print("Best Score:", grid_search_svm.best_score_)
print("Best Parameters (Neural Network):", grid_search_mlp.best_params_)
print("Best Score:", grid_search_mlp.best_score_)
print("Best Parameters (KNN):", grid_search_knn.best_params_)
print("Best Score:", grid_search_knn.best_score_)
print("Best Parameters (Naive Bayes):", grid_search_nb.best_params_)
print("Best Score:", grid_search_nb.best_score_)
print("Best Parameters (XGBoost):", grid_search_xgb.best_params_)
print("Best Score:", grid_search_xgb.best_score_)

best_scores = {
    'Logistic Regression': 0.687,
    'Decision Tree': 0.855,
    'Random Forest': 0.871,
    'AdaBoost': 0.512,
    'SVM': 0.689,
    'Neural Network': 0.749,
    'KNN': 0.685,
    'Naive Bayes': 0.667,
    'XGBoost': 0.869
}

plt.figure(figsize=(12, 6))
plt.bar(best_scores.keys(), best_scores.values(), color='skyblue')
plt.xlabel('Classifier')
plt.ylabel('Best Cross-Validation Accuracy Score')
plt.title('Best Cross-Validation Scores for Different Classifiers')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(axis='y')

for i, (classifier, score) in enumerate(best_scores.items()):
    plt.text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

random_seed = 42
cv_folds = 10

cv_scores_logistic = cross_val_score(LogisticRegression(max_iter=1000, random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Logistic Regression):", cv_scores_logistic)
print("Mean Accuracy:", cv_scores_logistic.mean())

cv_scores_tree = cross_val_score(DecisionTreeClassifier(random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Decision Tree):", cv_scores_tree)
print("Mean Accuracy:", cv_scores_tree.mean())

cv_scores_rf = cross_val_score(RandomForestClassifier(random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Random Forest):", cv_scores_rf)
print("Mean Accuracy:", cv_scores_rf.mean())

cv_scores_adaboost = cross_val_score(AdaBoostClassifier(random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (AdaBoost):", cv_scores_adaboost)
print("Mean Accuracy:", cv_scores_adaboost.mean())

cv_scores_svm = cross_val_score(SVC(random_state=random_seed, probability=True), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (SVM):", cv_scores_svm)
print("Mean Accuracy:", cv_scores_svm.mean())

cv_scores_mlp = cross_val_score(MLPClassifier(max_iter=1000, random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Neural Network):", cv_scores_mlp)
print("Mean Accuracy:", cv_scores_mlp.mean())

cv_scores_knn = cross_val_score(KNeighborsClassifier(), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (KNN):", cv_scores_knn)
print("Mean Accuracy:", cv_scores_knn.mean())

cv_scores_nb = cross_val_score(GaussianNB(), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Naive Bayes):", cv_scores_nb)
print("Mean Accuracy:", cv_scores_nb.mean())

cv_scores_xgb = cross_val_score(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (XGBoost):", cv_scores_xgb)
print("Mean Accuracy:", cv_scores_xgb.mean())

random_seed = 42
cv_folds = 10

classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_seed),
    'Decision Tree': DecisionTreeClassifier(random_state=random_seed),
    'Random Forest': RandomForestClassifier(random_state=random_seed),
    'AdaBoost': AdaBoostClassifier(random_state=random_seed),
    'SVM': SVC(random_state=random_seed, probability=True),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=random_seed),
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_seed)
}

cv_results = {}
for name, model in classifiers.items():
    cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')
    cv_results[name] = cv_scores

cv_df = pd.DataFrame(cv_results)

plt.figure(figsize=(12, 8))
sns.boxplot(data=cv_df, orient='h', palette='Set2')
plt.title('Cross-Validation Scores for Different Classifiers')
plt.xlabel('Accuracy')
plt.ylabel('Classifier')
plt.grid(True)
plt.show()

print(cv_df.mean().sort_values(ascending=False))

# Now with the another dataset heart_failure_prediction_dataset we showed results using this:
# data visualization
# data Preprocessing
# data splitting
# data normalization
# using the base classifiers models
# printing ROC curves for all models
# printing confusion matrix for all models
# Hyperparameter Tuning with GridSearchCV for Various Classifiers
# Including the Stacking Classifier and ROC Curves
# Evaluating all models with varying parameter sets
# K-Fold Cross-Validation for Multiple Classifiers and plot

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

# column_names = [
#     'Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina',
#     'oldpeak', 'ST_Slope', 'HeartDisease'
# ]

# Loading the datasets
heart_failure_disease_data = pd.read_csv('/content/heart_failure_prediction_dataset.csv', header=0)
second_data = heart_failure_disease_data.dropna()
print(second_data.head())

print(second_data.info())

print(second_data.describe())

# Handling the features
categorical_columns = second_data.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_columns)

# Encoding the categorical columns using LabelEncoder
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    second_data[col] = le.fit_transform(second_data[col])
    label_encoders[col] = le

# Here handling the missing values
second_data.fillna(second_data.median(), inplace=True)

# Calculating the correlation matrix
correlation_matrix = second_data.corr()
print("\nCorrelation Matrix:")
print(correlation_matrix)

# Data preprocessing
# Here seperating the target feature
target_column = 'HeartDisease'
X = second_data.drop(columns=[target_column])
y = second_data[target_column]

# Splitting the data into training and temporary sets
# Splitting the temporary set into validation and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# here using the Data Normalization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Initializing the classifiers
logistic_model = LogisticRegression()
tree_model = DecisionTreeClassifier(random_state=42)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)
svm_model = SVC(kernel='linear')
neural_network_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)
knn_model = KNeighborsClassifier(n_neighbors=5)
nb_model = GaussianNB()
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Fiting the models
logistic_model.fit(X_train, y_train)
tree_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
adaboost_model.fit(X_train, y_train)
svm_model.fit(X_train_scaled, y_train)
neural_network_model.fit(X_train_scaled, y_train)
knn_model.fit(X_train, y_train)
nb_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# predicting for the each model
predictions_logistic = logistic_model.predict(X_test)
predictions_tree = tree_model.predict(X_test)
predictions_rf = rf_model.predict(X_test)
predictions_adaboost = adaboost_model.predict(X_test)
predictions_svm = svm_model.predict(X_test_scaled)
nn_model_predictions = neural_network_model.predict(X_test_scaled)
knn_predictions = knn_model.predict(X_test)
nb_predictions = nb_model.predict(X_test)
xgb_predictions = xgb_model.predict(X_test)

# Printing the evaluation metrics
def evaluate_model(y_true, predictions, model_name):
    print(f"\nModel: {model_name}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, predictions))
    print("Classification Report:")
    print(classification_report(y_true, predictions))
    print("Accuracy:", accuracy_score(y_true, predictions))
    print("Mean Squared Error (MSE):", mean_squared_error(y_true, predictions))
    print("R² Score:", r2_score(y_true, predictions))

evaluate_model(y_test, predictions_logistic, "Logistic Regression")

evaluate_model(y_test, predictions_tree, "Decision Tree")

evaluate_model(y_test, predictions_rf, "Random Forest")

evaluate_model(y_test, predictions_adaboost, "AdaBoost")

evaluate_model(y_test, predictions_svm, "Support Vector Machine")

evaluate_model(y_test, nn_model_predictions, "Neural Network")

evaluate_model(y_test, knn_predictions, "K-Nearest Neighbors")

evaluate_model(y_test, nb_predictions, "Naive Bayes")

evaluate_model(y_test, xgb_predictions, "XGBoost")

def train_evaluate_model(model, X_train, y_train, X_test, y_test, model_name):

    start_train_time = time.time()
    model.fit(X_train, y_train)
    end_train_time = time.time()
    training_time = end_train_time - start_train_time

    start_test_time = time.time()
    predictions = model.predict(X_test)
    end_test_time = time.time()
    testing_time = end_test_time - start_test_time

    print(f"Model: {model_name}")
    print(f"Training Time: {training_time:.4f} seconds")
    print(f"Testing Time: {testing_time:.4f} seconds")

train_evaluate_model(logistic_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Logistic Regression')

train_evaluate_model(tree_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Decision Tree Classifier')

train_evaluate_model(rf_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Random Forest Classifier')

train_evaluate_model(adaboost_model, X_train_scaled, y_train, X_test_scaled, y_test, 'AdaBoost Classifier')

train_evaluate_model(svm_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Support Vector Machines')

train_evaluate_model(neural_network_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Neural Networks')

train_evaluate_model(knn_model, X_train_scaled, y_train, X_test_scaled, y_test, 'K-Nearest Neighbours')

train_evaluate_model(nb_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Naive Bayes')

train_evaluate_model(xgb_model, X_train_scaled, y_train, X_test_scaled, y_test, 'Extreme Gradient Boosting')

# Comparison of Model Accuracies
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'Neural Network', 'KNN', 'Naive Bayes', 'XGBoost']
accuracies = [0.87, 0.74, 0.89, 0.87, 0.86, 0.86, 0.71, 0.86, 0.84]

plt.figure(figsize=(10, 5))
sns.barplot(x=models, y=accuracies, palette='viridis')
plt.title('Comparison of Model Accuracies')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.show()

models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'Neural Network', 'KNN', 'Naive Bayes', 'XGBoost']
mse_scores = [0.12, 0.25, 0.10, 0.12, 0.13, 0.13, 0.28, 0.13, 0.15]
r2_scores = [0.48, -0.05, 0.57, 0.48, 0.45, 0.42, -0.20, 0.45, 0.36]

plt.figure(figsize=(10, 5))
plt.plot(models, mse_scores, marker='o', linestyle='-', color='b', label='MSE Score')
plt.title('MSE Scores by Model')
plt.xlabel('Model')
plt.ylabel('MSE Score')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(models, r2_scores, marker='o', linestyle='-', color='g', label='R² Score')
plt.title('R² Scores by Model')
plt.xlabel('Model')
plt.ylabel('R² Score')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'Neural Network', 'KNN', 'Naive Bayes', 'XGBoost']
training_times = [0.0078, 0.0049, 0.2235, 0.2068, 0.0204, 7.1418, 0.0051, 0.0050, 0.1415]
testing_times = [0.0005, 0.0004, 0.0085, 0.0163, 0.0014, 0.0007, 0.0225, 0.0019, 0.0023]

plt.figure(figsize=(12, 6))
plt.plot(models, training_times, marker='o', linestyle='-', color='black', label='Training Time')
plt.plot(models, testing_times, marker='s', linestyle='--', color='orange', label='Testing Time')
plt.title('Training and Testing Times for Various Models')
plt.xlabel('Model')
plt.ylabel('Time (seconds)')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, auc
def plot_roc_curve(models, X_test, y_test):
    plt.figure(figsize=(10, 8))

    for model_name, model in models.items():
        if hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X_test)[:, 1]
        else:
            y_score = model.decision_function(X_test)

        fpr, tpr, _ = roc_curve(y_test, y_score)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Different Models')
    plt.legend(loc="lower right")
    plt.grid()
    plt.show()

models_dict = {
    'XGBoost': xgb_model,
    'Logistic Regression': logistic_model,
    'Decision Tree Classifier': tree_model,
    'Random Forest Classifier': rf_model,
    'K-Nearest Neighbours': knn_model,
    'Neural Networks': neural_network_model,
    'Support Vector Machines': svm_model,
    'AdaBoost Classifier': adaboost_model
}

plot_roc_curve(models_dict, X_test, y_test)

# Here plotting the confusion matrix
def plot_confusion_matrix_only(model, X_test, y_test, model_name='Model'):

    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)
    plt.title(f'Confusion Matrix: {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

classifiers = [
    (logistic_model, "Logistic Regression"),
    (tree_model, "Decision Tree"),
    (rf_model, "Random Forest"),
    (adaboost_model, "AdaBoost"),
    (svm_model, "Support Vector Machine"),
    (neural_network_model, "Neural Network"),
    (knn_model, "K-Nearest Neighbours"),
    (nb_model, "Naive Bayes"),
    (xgb_model, "XGBoost")
]

for model, name in classifiers:
    X_test_data = X_test_scaled if name in ['Support Vector Machine', 'Neural Network'] else X_test
    plot_confusion_matrix_only(model, X_test_data, y_test, model_name=name)

base_models = [
    ('logistic', LogisticRegression()),
    ('tree', DecisionTreeClassifier(random_state=42)),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('adaboost', AdaBoostClassifier(n_estimators=100, random_state=42)),
    ('svm', SVC(kernel='linear', probability=True)),
    ('neural_net', MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)),
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('nb', GaussianNB()),
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))
]

meta_model = LogisticRegression()
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)
stacking_model.fit(X_train_scaled, y_train)
stacking_predictions = stacking_model.predict(X_test_scaled)
evaluate_model(y_test, stacking_predictions, 'Stacking Classifier')

def plot_roc_curve(models, X_test, y_test):
    plt.figure(figsize=(10, 8))
    for model_name, model in models.items():
        if hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X_test)[:, 1]
        else:
            y_score = model.decision_function(X_test)

        fpr, tpr, _ = roc_curve(y_test, y_score)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Different Models')
    plt.legend(loc="lower right")
    plt.grid()
    plt.show()

models_dict = {
    'XGBoost': xgb_model,
    'Logistic Regression': logistic_model,
    'Decision Tree Classifier': tree_model,
    'Random Forest Classifier': rf_model,
    'K-Nearest Neighbours': knn_model,
    'Neural Networks': neural_network_model,
    'Support Vector Machines': svm_model,
    'AdaBoost Classifier': adaboost_model
}

plot_roc_curve(models_dict, X_test_scaled, y_test)

logistic_param_sets = [
    {'C': 1.0, 'solver': 'lbfgs'},
    {'C': 0.5, 'solver': 'liblinear'},
    {'C': 2.0, 'solver': 'saga'}
]

tree_param_sets = [
    {'max_depth': 10, 'min_samples_split': 2},
    {'max_depth': 20, 'min_samples_split': 4},
    {'max_depth': None, 'min_samples_split': 10}
]

rf_param_sets = [
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 20},
    {'n_estimators': 300, 'max_depth': None}
]

adaboost_param_sets = [
    {'n_estimators': 50, 'learning_rate': 1.0},
    {'n_estimators': 100, 'learning_rate': 0.5},
    {'n_estimators': 200, 'learning_rate': 0.1}
]

svm_param_sets = [
    {'kernel': 'linear', 'C': 1.0},
    {'kernel': 'rbf', 'C': 0.5, 'gamma': 'scale'},
    {'kernel': 'poly', 'C': 2.0, 'degree': 3}
]

nn_param_sets = [
    {'hidden_layer_sizes': (100,), 'max_iter': 200},
    {'hidden_layer_sizes': (50, 50), 'max_iter': 500},
    {'hidden_layer_sizes': (100, 50), 'max_iter': 1000}
]

knn_param_sets = [
    {'n_neighbors': 5},
    {'n_neighbors': 10},
    {'n_neighbors': 20}
]

nb_param_sets = [
    {'var_smoothing': 1e-9},
    {'var_smoothing': 1e-8},
    {'var_smoothing': 1e-7}
]

xgb_param_sets = [
    {'n_estimators': 100, 'max_depth': 3},
    {'n_estimators': 200, 'max_depth': 6},
    {'n_estimators': 300, 'max_depth': 10}
]

def evaluate_param_sets(model_class, param_sets, model_name, scaled=True):
    results = []
    for params in param_sets:
        model = model_class(**params)

        start_train_time = time.time()
        if scaled:
            model.fit(X_train_scaled, y_train)
            predictions = model.predict(X_test_scaled)
        else:
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)
        end_train_time = time.time()
        training_time = end_train_time - start_train_time

        start_test_time = time.time()
        end_test_time = time.time()
        testing_time = end_test_time - start_test_time

        accuracy = accuracy_score(y_test, predictions)
        mse = mean_squared_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)

        results.append({
            'Parameters': params,
            'Accuracy': accuracy,
            'MSE': mse,
            'R² Score': r2,
            'Training Time': training_time,
            'Testing Time': testing_time
        })

    results_df = pd.DataFrame(results)
    print(f"Results for {model_name}:")
    print(results_df)

    results_df.plot(x='Parameters', y=['Accuracy', 'MSE', 'R² Score'], kind='bar', figsize=(12, 6), title=f'{model_name} Metrics by Parameter Set')
    plt.xticks(rotation=0)
    plt.grid(axis='y')
    plt.show()

evaluate_param_sets(LogisticRegression, logistic_param_sets, 'Logistic Regression', scaled=True)

evaluate_param_sets(DecisionTreeClassifier, tree_param_sets, 'Decision Tree', scaled=False)

evaluate_param_sets(RandomForestClassifier, rf_param_sets, 'Random Forest', scaled=False)

evaluate_param_sets(AdaBoostClassifier, adaboost_param_sets, 'AdaBoost', scaled=False)

evaluate_param_sets(SVC, svm_param_sets, 'Support Vector Machine', scaled=True)

evaluate_param_sets(MLPClassifier, nn_param_sets, 'Neural Network', scaled=True)

evaluate_param_sets(KNeighborsClassifier, knn_param_sets, 'K-Nearest Neighbours', scaled=False)

evaluate_param_sets(GaussianNB, nb_param_sets, 'Naive Bayes', scaled=False)

evaluate_param_sets(XGBClassifier, xgb_param_sets, 'Extreme Gradient Boosting', scaled=True)

param_grid_logistic = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear']
}

grid_search_logistic = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_logistic, cv=5, scoring='accuracy')
grid_search_logistic.fit(X_train, y_train)

param_grid_tree = {
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'criterion': ['gini', 'entropy']
}

grid_search_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_tree, cv=5, scoring='accuracy')
grid_search_tree.fit(X_train, y_train)

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10]
}

grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train, y_train)

param_grid_adaboost = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1]
}

grid_search_adaboost = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid_adaboost, cv=5, scoring='accuracy')
grid_search_adaboost.fit(X_train, y_train)

param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

grid_search_svm = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5, scoring='accuracy')
grid_search_svm.fit(X_train, y_train)

param_grid_mlp = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01]
}

grid_search_mlp = GridSearchCV(MLPClassifier(max_iter=1000), param_grid_mlp, cv=5, scoring='accuracy')
grid_search_mlp.fit(X_train, y_train)

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')
grid_search_knn.fit(X_train, y_train)

param_grid_nb = {
    'var_smoothing': np.logspace(-9, -3, 7)
}

grid_search_nb = GridSearchCV(GaussianNB(), param_grid_nb, cv=5, scoring='accuracy')
grid_search_nb.fit(X_train, y_train)

param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid_search_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), param_grid_xgb, cv=5, scoring='accuracy')
grid_search_xgb.fit(X_train, y_train)

print("Best Parameters (Logistic Regression):", grid_search_logistic.best_params_)
print("Best Score:", grid_search_logistic.best_score_)
print("Best Parameters (Decision Tree):", grid_search_tree.best_params_)
print("Best Score:", grid_search_tree.best_score_)
print("Best Parameters (Random Forest):", grid_search_rf.best_params_)
print("Best Score:", grid_search_rf.best_score_)
print("Best Parameters (AdaBoost):", grid_search_adaboost.best_params_)
print("Best Score:", grid_search_adaboost.best_score_)
print("Best Parameters (SVM):", grid_search_svm.best_params_)
print("Best Score:", grid_search_svm.best_score_)
print("Best Parameters (Neural Network):", grid_search_mlp.best_params_)
print("Best Score:", grid_search_mlp.best_score_)
print("Best Parameters (KNN):", grid_search_knn.best_params_)
print("Best Score:", grid_search_knn.best_score_)
print("Best Parameters (Naive Bayes):", grid_search_nb.best_params_)
print("Best Score:", grid_search_nb.best_score_)
print("Best Parameters (XGBoost):", grid_search_xgb.best_params_)
print("Best Score:", grid_search_xgb.best_score_)

best_scores = {
    'Logistic Regression': 0.841,
    'Decision Tree': 0.815,
    'Random Forest': 0.863,
    'AdaBoost': 0.852,
    'SVM': 0.844,
    'Neural Network': 0.836,
    'KNN': 0.702,
    'Naive Bayes': 0.850,
    'XGBoost': 0.863
}

plt.figure(figsize=(12, 6))
plt.bar(best_scores.keys(), best_scores.values(), color='skyblue')
plt.xlabel('Classifier')
plt.ylabel('Best Cross-Validation Accuracy Score')
plt.title('Best Cross-Validation Scores for Different Classifiers')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(axis='y')

for i, (classifier, score) in enumerate(best_scores.items()):
    plt.text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

random_seed = 42
cv_folds = 10

cv_scores_logistic = cross_val_score(LogisticRegression(max_iter=1000, random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Logistic Regression):", cv_scores_logistic)
print("Mean Accuracy:", cv_scores_logistic.mean())

cv_scores_tree = cross_val_score(DecisionTreeClassifier(random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Decision Tree):", cv_scores_tree)
print("Mean Accuracy:", cv_scores_tree.mean())

cv_scores_rf = cross_val_score(RandomForestClassifier(random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Random Forest):", cv_scores_rf)
print("Mean Accuracy:", cv_scores_rf.mean())

cv_scores_adaboost = cross_val_score(AdaBoostClassifier(random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (AdaBoost):", cv_scores_adaboost)
print("Mean Accuracy:", cv_scores_adaboost.mean())

cv_scores_svm = cross_val_score(SVC(random_state=random_seed, probability=True), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (SVM):", cv_scores_svm)
print("Mean Accuracy:", cv_scores_svm.mean())

cv_scores_mlp = cross_val_score(MLPClassifier(max_iter=1000, random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Neural Network):", cv_scores_mlp)
print("Mean Accuracy:", cv_scores_mlp.mean())

cv_scores_knn = cross_val_score(KNeighborsClassifier(), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (KNN):", cv_scores_knn)
print("Mean Accuracy:", cv_scores_knn.mean())

cv_scores_nb = cross_val_score(GaussianNB(), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (Naive Bayes):", cv_scores_nb)
print("Mean Accuracy:", cv_scores_nb.mean())

cv_scores_xgb = cross_val_score(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_seed), X, y, cv=cv_folds, scoring='accuracy')
print("K-Fold Cross-Validation Scores (XGBoost):", cv_scores_xgb)
print("Mean Accuracy:", cv_scores_xgb.mean())

random_seed = 42
cv_folds = 10

classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_seed),
    'Decision Tree': DecisionTreeClassifier(random_state=random_seed),
    'Random Forest': RandomForestClassifier(random_state=random_seed),
    'AdaBoost': AdaBoostClassifier(random_state=random_seed),
    'SVM': SVC(random_state=random_seed, probability=True),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=random_seed),
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_seed)
}

cv_results = {}
for name, model in classifiers.items():
    cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')
    cv_results[name] = cv_scores

cv_df = pd.DataFrame(cv_results)

plt.figure(figsize=(12, 8))
sns.boxplot(data=cv_df, orient='h', palette='Set2')
plt.title('Cross-Validation Scores for Different Classifiers')
plt.xlabel('Accuracy')
plt.ylabel('Classifier')
plt.grid(True)
plt.show()

print(cv_df.mean().sort_values(ascending=False))